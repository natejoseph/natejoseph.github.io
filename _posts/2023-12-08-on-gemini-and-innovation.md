---
layout: post
title: "Is Google's Gemini Changing Artificial Intelligence? A Student Reaction"
date: 2023-12-08 01:20:00 -0400
author: Nate Joseph
editor: Axel Cardona-Olivero
editor_link: https://www.linkedin.com/in/axel-cardona-olivero-26a69a266/
categories: blog
excerpt: My own personal take on Google DeepMind's Gemini model
---

### OpenAI's <i>ChatGPT</i> is Released to the Public

In 2023, OpenAI released <i>ChatGPT</i> to the public. Large language models (LLM) provided <i>ChatGPT</i> with the ability to generate text at any length, in any format, and at varying levels of detail. As a computer engineer, I was fascinated by <i>ChatGPT</i>’s capabilities and I wanted to know more.

<i>ChatGPT</i> stands for, chat generative pre-trained transformer, which is a technology engineered by programmers, at OpenAI. Since <i>ChatGPT</i> is still new to the public, insider information can only be found through artificial intelligence (AI) educational courses. So, I enrolled in the most updated courses on AI, and here’s what those courses were.

I received my first introduction to AI development two summers ago, through LinkedIn Learning’s ‘Explore a Career in Machine Learning Engineering learning path’ (ECMLE), a LinkedIn service I have access to as a student. ECMLE taught me the basic concepts for understanding AI’s use in application development. Currently, I am using what I learned, from ECMLE, to build a <a href="../../../../assets/pdfs/CalorieChecker_UMComputingDayPoster.pdf">Calorie Checker</a> application. Naturally, my interest in AI grew, and this is when I decided to take my next course.

I noticed that a large part of <i>ChatGPT</i>’s success derived from its ability to understand and interpret human ideas and emotions. But I didn’t quite understand the technology, so as generative models continued to burst in popularity I took advantage of Google Cloud Skills Boost’s ‘Generative AI learning path’ to further my knowledge.

From then, I took courses on artificial intelligence and neural networking, machine-learning processes modeled after the human brain. Generative AI models like <i>GPT-4</i> are forms of neural networks modeled and trained for specific purposes. These courses taught me how to use AI to solve human problems, even when contingencies such as unique needs, situations, and experiences, were involved. I continue to self-educate each day, because of the constant reminder that AI is always improving. I plan on getting ahead in AI information so that I can be a developer of the technology that is taking the world by storm.

### Google's <i>Gemini</i> Introduces Multimodal Technology

As finals week approaches for computer engineering majors, the campus at the University of Miami is quiet. During this time, I naturally gravitate toward news on tech, and recently artificial intelligence has been trending. During a study break, I stumbled on news that Google has released a new artificial intelligence model family, called Gemini.

Since Google’s Input Output (I/O) conference, in May 2023, reports of Gemini’s specifications, capabilities, and release date have been unclear. Just two days ago, I got an update from the TLDR AI newsletter, claiming that Gemini would not be released until January 2024. Since I only knew what rumors were saying about Gemini, I dove into some research.

Google’s <i>Gemini</i> differs from OpenAI’s <i>ChatGPT</i> by the architecture of the neural network itself. By training the model on multiple types of data at once, <i>Gemini</i> introduces a new “multimodal” form of data processing that interleaves textual, audio, and visual inputs, and can produce interleaved images and text as output. This means that AI’s interpretive technology has more abilities and answers for users of Google’s <i>Gemini</i>.

Google’s <i>Gemini</i> is offered in families of three different sizes: <i>Gemini Ultra</i>, <i>Gemini Pro</i>, and <i>Gemini Nano</i>. In direct competition with OpenAI’s <i>GPT-4</i>, <i>Gemini Ultra</i>; optimized for performance and adaptability, <i>Gemini Pro</i>; and finally, the ultra-efficient model currently incorporated in Google’s <i>Pixel 8 Pro</i>, <i>Gemini Nano</i>.

The training of these models, on Google’s TPUv5e and TPUv4, required massive innovations in algorithms, datasets, and infrastructures, which allowed the joint training of these natively multimodal models. Announced alongside the model family is the newest TPUv5p, which should prove to allow for further technological innovations in the future. The invention of Gemini answers the question of whether the mixing of inputs can produce a model with strong capabilities in each domain when compared to single-domain models.

“With an [accuracy] score of 90.0%, <i>Gemini Ultra</i> is the first model to outperform human experts on MMLU (massive multitask language understanding), which uses a combination of 57 subjects such as math, physics, history, law, medicine and ethics for testing both world knowledge and problem-solving abilities” (The Keyword).

In the future, what sort of innovation can we expect from multimodal AI? Now that multiple forms of input can be artificially understood at once, we step into artificial general information (AGI) territory. We as humans intake information into all five of our senses at once, and the idea of a single AI that can intake and interpret more and more types of information that we as humans learn to create and distribute virtually brings about so many new questions.

How long is it until we have AI that can interpret and produce scents? Or compare the textures of different materials through touch? The idea of interleaving multiple forms of information input and output at once, and the proof that such an idea can be artificially replicated in a form that is competitive truly does mark the next level of artificial intelligence innovation.

### Learning From Artificial Intelligence

In only a year, the world has witnessed rapid change in information processing technology. This is why I dedicate myself to innovation in what I learn every day. As AI improves and becomes a larger part of our daily lives, I envision a future that does not yet exist; a future that constantly changes as we continue to advance and explore the scientific limits of our universe. For now, I stick to learning the skills that will push the world to the next level.

#### Hey reader,

I asked Google <i>Bard</i> for suggestions about this blog post and it recommended that I provide a call to action and links to the work discussed. So, for more information about the new technology, click <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">here</a> for the technical report, <a href="https://blog.google/technology/ai/google-gemini-ai/">here</a> for the blog post, and <a href="https://deepmind.google/technologies/gemini/#introduction">here</a> for videos and more information.

I would love it if you shared and interacted with this post; send me an email; connect with me on Linkedin! Want to work together? <a href="../../../../assets/pdfs/Nathaniel_Joseph_resume.pdf">Here</a>'s my resume. I’m a student at the University of Miami and I would love to help you.
