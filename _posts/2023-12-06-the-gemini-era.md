---
layout: post
title: "Thoughts on The Gemini Era and Natively Multimodal AI"
date: 2023-12-07 06:40:00 -0400
author: Nate Joseph
categories: blog
excerpt: My own personal take on Google DeepMind's Gemini model
---

While (taking a break from) studying for my Artificial Intelligence cumulative final approaching this Friday, I stumbled across the news that Google has released its new Gemini AI model family, Gemini, today. There have been varying reports of Gemini's specifications, release date, and capabilities across multiple sources on the internet since its announcement at I/O 2023. Even just two days ago, I received news from the TLDR (AI) newsletter that Gemini's launch was being delayed until January (which is partly true, to my knowledge). So of course, once I discovered its release, I had to see for myself what all the hype was about, and I was not disappointed.

I've dipped my toes into AI development multiple times over my career as a student. My first introduction to AI was through a learning path on Linkedin Learning from two summers ago. I was learning the basics of AI development and concepts in order to understand their usage in application development for an app I was (and still am) working on, Calorie Checker. We were planning the incorportation of image recognition into a Unity application, and as naive students, we used the most accessible resource to us, recommended to us by our scrum master, as our school provided Linkedin Learning for students. While the knowledge was great for sparking an interest in the subject matter, it did little to actually teach a foundational understanding of the topics discussed, and left me with a very shallow understanding of what AI is, and how useful it can be. It culminated with the development of chatbots with Microsoft Azure, which I looked at as boring and redundant. What's the point of making another chatbot when there already exists some on the internet? The idea of how good they could actually be and the potential of LLMs was never even a thought in my mind back then. Boy, was I wrong.

When OpenAI first released ChatGPT to the public a year ago, I was one of the first adopters of it, and found its usefulness instantly in writing a final report in a time when professors didn't even know it was a thing to be aware of. I was fascinated by the capabilities of the LLM, and an interest in AI research took root. In that spring semester, I enrolled myself in the most complex AI courses I could take for the upcoming fall, and watched firsthand as the AI industry exploded. As time passed, the more I anticipated takig those courses in the fall, and I took it upon myself to continue self-educating in the meantime, through free resources such as Google Cloud Skills Boost's Generative AI learning path.

When this past semester started, I was fascinated by all that I didn't know about AI, and learned so much about it. The study of artificial intelligence has been occurring for decades. Concepts of what we know as AI today have been studied since circa 1950. So, while I was learning the history, fundamentals, theories, and applications of AI in my Introduction to Artificial Intelligence course, I was learning the modern usage of it through machine learning models and neural networking in my Neural Networks course. I gained proficiency applying AI and thinking the way an AI architect would to solve my problems, understanding and experience using different resources and tools fit my needs in different scenarios, and knowledge of where to source and how to understand further information of any AI innovations I could ever need to use. As the semester comes to a close, I decided to take a chunk out of my last days of studying for finals to write this blog post on such an important innovation I will use in the future, Gemini. I read the technical report, the blog post, and watched all of the YouTube demonstration videos and am so fascinated by what I have learned.

The Gemini 1.0 model family comes in three sizes: Ultra, which is in direct competition (and beats in many regards) with OpenAI's GPT-4; Pro, a "performance optimized" version which is optimized to be used across a multitude of formats and devices, and is currently incorportated into Google's Bard AI as of today; and Nano, a "best-in-class," ultra-efficient model of Gemini which is meant to be run on-device, trained by distilling from the larger models. Nano comes in two versions, Nano-1 and Nano-2, the latter of which has almost double the parameters as the former to target "low and high memory devices respectively."

The thing that makes these models so unique is that they are trained to interleave textual, audio, and visual inputs, and can produce interleave images and text as outputs. The training of these models, on Google's TPUv5e and TPUv4 (they also just announced the new TPUv5p today) required massive innovations in algorithms, datasets, and infrastructures in order to allow the joint training of these natively multimodal models. The question of whether all of this effort is worth it; whether the mixing of inputs can produce a model with strong capabilities in each domain even when compared to models that are tailored to single domains are answered with the Gemini family, and this answer is a resounding yes.

"With a score of 90.0%, Gemini Ultra is the first model to outperform human experts on MMLU (massive multitask language understanding), which uses a combination of 57 subjects such as math, physics, history, law, medicine and ethics for testing both world knowledge and problem-soling abilities."

Across a range of different industry standard benchmarks, including MMLU and MATH, Gemini Ultra performs better than GPT-4 on all except for HellaSwag (a commonsense reasoning benchmark). While evaluation on benchmarks is challenging due to data contamination, by evaluating on evaluation datasets that are recently released or generated from non-web sources, DeepMind evaluators worked to mitigate that possibility. Regardless, benchmarks provide indication of a model's capabilities.

Evaluations scored from Gemini's family of models set a new SotA across text, image, audio, and video benchmarks alike, beating GPT-4 in general in every criteria reported. Gemini can also combine with other techniques like search and tool-use, allowing Google to create powerful and complex systems such as AlphaCode 2, an agent tuned on competitive programming and performs better than about 85% of competitors, where as the previous version, AlphaCode only outformed nigh 50%.

Surprisingly, Gemini Pro does something specific better than its Ultra big sibling, and that is generating English image descriptions (using the XM-3600 benchmark), on which Gemini Pro scored .7 higher than Ultra in a 4-shot setting. For every other of the seven languages tested, Ultra beat out Pro, and both models beat the previous state of the art model, Google PaLI-X.

Multimodality is the selling point of the models, and there is a unique demonstration of image and audio prompts with text outputs, where testers created an cooking scenario using images and audio files, which Gemini was able to process in order to assist a DeepMind employee in cooking a veggie omelet and serving it once it was ready. Never before have I seen an AI that can be interacted with in such a fashion, and I can't wait to see where the technology goes in the next few years.

How can we know what sorts of innovations we can expect from multimodal AI in the future? The idea that we can input all sorts of information in different formats into an AI that can interpret all of it is a step closer to AGI. We as humans intake information into all five of our senses at once, and the idea of a single AI that can intake and interpret more and more types of information that we as humans learn to create and distribute on the internet begets so many new questions. How long is it until we have AI that can interpret and produce scents? Or compare the textures of a new material by feeling? The idea of interleaving multiple forms of information input and output at once, and the proof that such an idea can be artificially replicated in a form superior than ideas of post-processing differing information truly does mark the next level of artificial intelligence innovation.

In only a year's time, we've all been witness to a rapid change in information processing in the digital age. Something that was previously only explored in a mainstream fashion via the movies is now being used and understood by more humans than ever is nothing short of amazing and is why I dedicate myself to innovation daily. I envision a future where we as humans explore technological advancements to the best of our abilities as a species, and create and use things I could never have even imagined. For now, I wait to be able to use Gemini Ultra in the coming months and continue to equip myself with the tools necessary to take advantage of innovations as technology continues to evolve.

When asking Bard (now equipped with Gemini Pro) for insights on this blog post, it recommended providing links for easy access and adding a call to action, so I'll do just that! The technical report can be found <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf">here</a>, blog post <a href="https://blog.google/technology/ai/google-gemini-ai/">here</a>, and webpage with videos and more information <a href="https://deepmind.google/technologies/gemini/#introduction">here</a>. With that being said, I would love for readers to share and interact with me via email or Linkedin! My resume can also be viewed and downloaded <a href="https://natejoseph.me/assets/pdfs/Nathaniel_Joseph_resume.pdf">here</a>. I'm a student, but if you think I can do something for you, we should connect and figure something out.
